- name: install kubernetes repo
  yum_repository:
    name: kubernetes
    description: kubernetes repo
    baseurl: "{{kubernetes_repo}}"
    gpgcheck: yes
    gpgkey: "{{kubernetes_gpgkey}}"
    file: kubernetes
  become: yes

- name: install packages
  yum:
    name: "{{kube_packages}}"
    state: present
  become: yes

- name: get cgroup info
  shell: docker info | grep "Cgroup Driver" | awk '{print $3}'
  register: cgroup_info
  become: yes

- name: enable the proper cgroup driver
  replace:
    path: "{{kubeadm_conf}}"
    regexp: "systemd"
    replace: "{{cgroup_info.stdout}}"
  register: cgroup_kube
  become: yes

- name: set dns server inside service cidr
  replace:
    path: "{{kubeadm_conf}}"
    regexp: 10.96.0.10
    replace: "{{dns_svc_cidr}}"
  register: dns_kube
  become: yes

- name: reload kubelet
  systemd: daemon-reload=yes
  become: yes
  when: cgroup_kube.changed or dns_kube.changed

- name: stop kubelet service
  service:
    name: kubelet
    state: stopped
  become: yes
  when: cgroup_kube.changed or dns_kube.changed

- name: start kubelet service
  service:
    name: kubelet
    state: started
    enabled: yes
  become: yes

- name: check that kubelet is failing
  shell: systemctl status kubelet | grep Active
  register: kubelet_status
  become: yes

- debug:
    msg: "{{kubelet_status}}"

- name: check to see if kubernetes is up
  shell: kubectl cluster-info | grep -o "KubeDNS.*is running"
  register: cluster_status

- debug:
    msg: "{{cluster_status}}"

- name: configure iptables bridge
  sysctl:
    name: "{{item}}"
    value: 1
    sysctl_set: yes
    state: present
    reload: yes
  with_items:
    - net.bridge.bridge-nf-call-iptables
    - net.bridge.bridge-nf-call-ip6tables
  become: yes

- name: deploy kubernetes with kubeadm
  shell: kubeadm init --pod-network-cidr={{pod_net_cidr}} --service-cidr={{service_cidr}}
  become: yes
  run_once: yes
  when: (kubelet_status.stdout.find("activating") != -1) or (cluster_status.stdout is not match("KubeDNS.*is running"))

- name: make kubeadm creds folder
  file:
    name: "{{ansible_env.HOME}}/.kube"
    state: directory

- name: copy admin.conf to kube conf
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ansible_env.HOME}}/.kube/config"
    owner: "{{ansible_user_id}}"
    group: "{{ansible_user_gid}}"
    remote_src: yes
  become: yes

- name: check clusterrolebinding
  shell: kubectl get clusterrolebinding --all-namespaces | grep canal-calico | awk '{print $1}'
  args:
    chdir: "{{ansible_env.HOME}}"
  register: rbac_deployed

- name: apply rbac config
  shell: kubectl apply -f {{rbac_config}}
  run_once: yes
  when: rbac_deployed.stdout != "canal-calico"

- name: check canal configmap
  shell: kubectl get configmap --all-namespaces | grep canal-config | awk '{print $2}'
  args:
    chdir: "{{ansible_env.HOME}}"
  register: canal_deployed

- name: download canal config
  get_url:
    url: "{{canal_config}}"
    dest: "{{canal_config_path}}"
  when: canal_deployed.stdout != "canal-config"

- name: update canal config
  replace:
    path: "{{canal_config_path}}"
    regexp: 10.244.0.0/16
    replace: "{{pod_net_cidr}}"
  when: canal_deployed.stdout != "canal-config"

- name: apply canal config
  shell: kubectl apply -f {{canal_config_path}}
  run_once: yes
  when: canal_deployed.stdout != "canal-config"

- name: check master status
  shell: kubectl get nodes --all-namespaces | grep master | awk '{print $2}'
  args:
    chdir: "{{ansible_env.HOME}}"
  register: master_status

- name: taint nodes
  command: kubectl taint nodes --all=true  node-role.kubernetes.io/master:NoSchedule-
  run_once: yes
  when: master_status.stdout == "NotReady"